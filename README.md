# Assignment-2.4

1.Hadoop in layman's term:
Hadoop is an open source, Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing environment. It is part of the Apache project sponsored by the Apache Software Foundation.
It is divided into two components namely:
                   a)Storage-HDFS
                   b)Processing-MapReduce,pig,hive
                   
2.Components of Hadoop framework:
The 3 core components of the Apache Software Foundation’s Hadoop framework are:

1. MapReduce – A software programming model for processing large sets of data in parallel
2. HDFS – The Java-based distributed file system that can store all kinds of data without prior organization.
3. YARN – A resource management framework for scheduling and handling resource requests from distributed applications.

In this blog we’ll take a shallow dive into the Hadoop Distributed File System and its significance and contribution in providing sturdiness to the Data residing on the Hadoop framework.

HDFS is the storage sheath of Hadoop. It takes care of storing data of petabyte scale.

Saturation makes it necessary to think laterally and marches towards scaling.

As, and when data, grows vigorously, it is constantly challenging the human perception of building and stacking data storage in the “vertical” form (i.e. accommodating data growth only on a single machine, the concept of “scaling up” was facing chronic saturation.)

3.Explain the reasons to learn Big data technologies
•Build new applications: Big data might allow a company to collect billions of real-time data points on its products, resources, or customers – and then repackage that data instantaneously to optimize customer experience or resource utilization. For example, a major US city is using MongoDB to cut crime and improve municipal services by collecting and analyzing geospatial data in real-time from over 30 different departments.
•Improve the effectiveness and lower the cost of existing applications: Big data technologies can replace highly-customized, expensive legacy systems with a standard solution that runs on commodity hardware. And because many big data technologies are open source, they can be implemented far more cheaply than proprietary technologies. For example, by migrating its reference data management application to MongoDB, a Tier 1 bank dramatically reduced the license and hardware costs associated with the proprietary relational database it previously ran, while also bringing its application into better compliance with regulatory requirements.
•Realize new sources of competitive advantage: Big data can help businesses act more nimbly, allowing them to adapt to changes faster than their competitors. For example, MongoDB allowed one of the largest Human Capital Management (HCM) solution providers to rapidly build mobile applications that integrated data from a wide variety of disparate sources.
•Increase customer loyalty: Increasing the amount of data shared within the organization – and the speed with which it is updated – allows businesses and other organizations to more rapidly and accurately respond to customer demand. For example, a top 5 global insurance provider, MetLife,used MongoDB to quickly consolidate customer information from over 70 different sources and provide it in a single, rapidly-updated view.
• Demand for Big Data skills is extremely high, and being able to prove your
expertise is of essence
• 64% of IT hiring managers rate skilled big data knowledge as having extremely high or high value when rating expertise of candidates; this is based on a survey by CompTIA.
• According to Forbes, the median advertised salary for professionals with Big Data expertise is $124,000 a year.
• IBM, Cisco, and Oracle together advertised 26,488 open positions that required Big Data expertise in the last twelve months.
